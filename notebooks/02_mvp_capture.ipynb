{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5823516c",
   "metadata": {},
   "source": [
    "MD # 02 Capture activations (MVP)\n",
    "MD Runs each prompt once, stores residual activations per layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f67daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved cache for Q__What_is_2___3__A\n",
      "saved cache for Q__If_I_have_five_apples_and_eat_two__ho\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "import re\n",
    "import torch, json\n",
    "from pathlib import Path\n",
    "import sys, pathlib, importlib\n",
    "sys.path.append(str(pathlib.Path('..').resolve()))   # let Python see project root\n",
    "\n",
    "from utils import set_seed, load_model, load_prompts\n",
    "\n",
    "\n",
    "class ActivationCache:\n",
    "    def __init__(self):\n",
    "        self.hidden: Dict[int, torch.Tensor] = {}\n",
    "\n",
    "    def hook(self, layer: int):\n",
    "        def _capture(_m, _inp, out):\n",
    "            # out can be a Tensor **or** a tuple whose first item is the tensor\n",
    "            hs = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            self.hidden[layer] = hs.detach()\n",
    "        return _capture\n",
    "\n",
    "\n",
    "def safe_slug(text: str, max_len: int = 40) -> str:\n",
    "    \"\"\"\n",
    "    Turn an arbitrary string into a Windows-safe filename.\n",
    "    Keeps A–Z, a–z, 0–9, dash and underscore; drops anything else.\n",
    "    \"\"\"\n",
    "    clean = re.sub(r\"[^0-9A-Za-z_-]\", \"_\", text)   # replace bad chars with _\n",
    "    return clean[:max_len].rstrip(\"_\")\n",
    "\n",
    "def forward_with_cache(model, ids) -> Tuple[torch.Tensor, 'ActivationCache']:\n",
    "    cache = ActivationCache()\n",
    "    hooks = [model.transformer.h[i].register_forward_hook(cache.hook(i))\n",
    "             for i in range(model.config.n_layer)]\n",
    "    logits = model(ids).logits\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    return logits, cache\n",
    "\n",
    "# ---- main execution ----\n",
    "set_seed(0)\n",
    "model, tok = load_model('gpt2')\n",
    "prompts = load_prompts('../data/reasoning_prompts.json')\n",
    "\n",
    "Path('../data/caches').mkdir(exist_ok=True)\n",
    "\n",
    "for p in prompts:\n",
    "    ids = tok(p, return_tensors='pt').input_ids.to(DEVICE)\n",
    "    _, cache = forward_with_cache(model, ids)\n",
    "    slug = safe_slug(p)\n",
    "    torch.save(cache.hidden, f'../data/caches/{slug}.pt')\n",
    "    print('saved cache for', slug)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
